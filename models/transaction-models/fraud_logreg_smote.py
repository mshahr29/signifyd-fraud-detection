# -*- coding: utf-8 -*-
"""Fraud_LogReg_SMOTE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r8JDNms3c_eEKxYfDXdUfDYq8XWPBiu-

# Base Model: Ecommerce Transactions Fraud Detection

## EDA
"""

#Importing libraries and the dataset
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn
import datetime
import calendar
import warnings

# Ignore warnings
warnings.filterwarnings('ignore')

# Read the datasets
user_info = pd.read_csv("Fraud_Data.csv")         # Users information
ip_country_mapping = pd.read_csv("IpAddress_to_Country.csv")  # Country from IP information

ip_country_mapping.head()

ip_country_mapping.info()

user_info.head()

user_info.info()

"""**Mapping/Merging fraud data with country via IP address**"""

ip_country_mapping.upper_bound_ip_address.astype("float")
ip_country_mapping.lower_bound_ip_address.astype("float")
user_info.ip_address.astype("float")

def IP_to_country(ip) :
    try :
        return ip_country_mapping.country[(ip_country_mapping.lower_bound_ip_address < ip)
                                &
                                (ip_country_mapping.upper_bound_ip_address > ip)].iloc[0]
    except IndexError :
        return "Unknown"

import os

# Define the directory path within Colab's file storage
directory = "/content/datasets_fraud"

# Check if the directory exists, if not, create it
if not os.path.exists(directory):
    os.makedirs(directory)

# country to each IP
user_info["IP_country"] = user_info.ip_address.apply(IP_to_country)

# saving
user_info.to_csv("/content/datasets_fraud/Fraud_data_with_country.csv", index=False)

# loading
user_info= pd.read_csv("/content/datasets_fraud/Fraud_data_with_country.csv")

user_info.head()

# Print summary statistics
print(user_info[["purchase_value", "age"]].describe())
print('*'*50)
# Print unique values and their frequencies
for column in ["source", "browser", "sex"]:
    print(user_info[column].value_counts())
    print('*'*50)

# Check for duplicates in the "user_id" column in user_info DataFrame
print("The user_id column includes {} duplicates".format(user_info.duplicated(subset="user_id", keep=False).sum()))

# Calculate duplicate rate based on unique device_id
dup_table = pd.DataFrame(user_info.duplicated(subset="device_id"))
dup_rate = dup_table.mean()
print("{}% of the dataset is comprised of transactions from a device_id that had been previously used".format(int(dup_rate * 1000) / 10))

# Calculate duplicate rate based on device_id with keep=False
dup_table2 = pd.DataFrame(user_info.duplicated(subset="device_id", keep=False))
dup_rate2 = dup_table2.mean()
print("{}% of the dataset is comprised of transactions from a device_id that had been previously used".format(int(dup_rate2 * 1000) / 10))

"""The code calculates two duplicate rates based on the device_id column.
The first rate considers only the subsequent occurrences of a duplicate device_id,
while the second considers all occurrences (first and subsequent).
This provides two different perspectives on the extent of device reuse in the dataset.
"""

device_duplicates = pd.DataFrame(user_info.groupby(by="device_id").device_id.count())
device_duplicates.rename(columns={"device_id": "freq_device"}, inplace=True)
device_duplicates.reset_index(level=0, inplace=True)
dupli = device_duplicates[device_duplicates.freq_device >1]
dupli

# Reading the Dataset
user_info = pd.read_csv("/content/datasets_fraud/Fraud_data_with_country.csv")

device_duplicates = pd.DataFrame(user_info.groupby(by = "device_id").device_id.count())
device_duplicates.rename(columns={"device_id": "freq_device"}, inplace=True)
device_duplicates.reset_index(level=0, inplace= True)

dupli = device_duplicates[device_duplicates.freq_device >1]
print("On average, when a device is used more than once it is used {mean} times, and the most used machine was used {maxi} times"
      .format(mean = int(dupli.freq_device.mean()*10)/10, maxi = int(dupli.freq_device.max()*10)/10))

dupli = device_duplicates[device_duplicates.freq_device >2]
print("On average, when a device is used more than twice it is used {mean} times"
      .format(mean = int(dupli.freq_device.mean()*10)/10, maxi = int(dupli.freq_device.max()*10)/10))

# Merge the device_duplicates with user_info
user_info = user_info.merge(device_duplicates, on="device_id")

# Calculate the proportion of fraud in the dataset
fraud_proportion = user_info["class"].mean() * 100
print("Proportion of fraud in the dataset: {:.1f}%".format(fraud_proportion))

user_info.describe()

import matplotlib.pyplot as plt
import seaborn as sns

# Create subplots
f, ax = plt.subplots(1, 2, figsize=(12, 6))

# Plot device frequency distribution for values less than 4
g1 = sns.distplot(user_info.freq_device[user_info.freq_device < 4], ax=ax[0])
g1.set(xticks=[1, 2, 3])

# Plot device frequency distribution for values greater than 2
g2 = sns.distplot(user_info.freq_device[user_info.freq_device > 2], ax=ax[1])
g2.set(xticks=range(0, 21, 2))

# Display the plots
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Set the figure size
plt.figure(figsize=(18, 6))

# Create subplots for bar plots
plt.subplot(1, 3, 1)
sns.barplot(x='source', y='class', data=user_info, ci=None)
plt.title('Fraud Proportion by Source')

plt.subplot(1, 3, 2)
sns.barplot(x='browser', y='class', data=user_info, ci=None)
plt.title('Fraud Proportion by Browser')

plt.subplot(1, 3, 3)
sns.barplot(x='sex', y='class', data=user_info, ci=None)
plt.title('Fraud Proportion by Sex')

# Adjust layout
plt.tight_layout()

# Show plot
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Set up the subplots
f2, ax2 = plt.subplots(3, 1, figsize=(24, 18))

# Plot purchase_value vs. class
sns.pointplot(x="purchase_value", y="class", data=user_info, ci=None, ax=ax2[0])
ax2[0].set_title("Purchase Value vs. Fraud Probability")

# Plot age vs. class
sns.pointplot(x="age", y="class", data=user_info, ci=None, ax=ax2[1])
ax2[1].set_title("Age vs. Fraud Probability")

# Plot freq_device vs. class
sns.pointplot(x="freq_device", y="class", data=user_info, ci=None, ax=ax2[2])
ax2[2].set_title("Frequency of Device Usage vs. Fraud Probability")

# Show the plots
plt.tight_layout()
plt.show()

user_info.head()

import seaborn as sns
import matplotlib.pyplot as plt

# Create a figure and axis
f3, ax3 = plt.subplots(1, 1, figsize=(24, 18))

# Plot a stacked bar plot for IP_country vs. class
sns.barplot(x="IP_country", y="class", data=user_info[:10], estimator=sum, ci=None, ax=ax3)

# Show the plot
plt.show()

# Filter IP_country value counts where count is greater than 1000
filtered_counts = user_info.IP_country.value_counts()[user_info.IP_country.value_counts() > 1000]

# Plot the filtered counts as a bar plot
filtered_counts.plot(kind="bar")
plt.xlabel("IP Country")
plt.ylabel("Frequency")
plt.title("IP Country Frequency (Counts > 1000)")
plt.show()

user_info.signup_time

"""## Feature Engineering"""

# --- 1 ---
# Categorisation column freq_device
# We see a clear correlation between freq_device and fraudulent activities. We are going to split freq_device into 7 categories
user_info.freq_device = user_info.freq_device.apply(lambda x:
                                                    str(x) if x < 5 else
                                                    "5-10" if x >= 5 and x <= 10 else
                                                    "11-15" if x > 10 and x <= 15 else
                                                    "> 15")

# Convert signup_time and purchase_time to datetime
user_info.signup_time = pd.to_datetime(user_info.signup_time, format='%Y-%m-%d %H:%M:%S')
user_info.purchase_time = pd.to_datetime(user_info.purchase_time, format='%Y-%m-%d %H:%M:%S')

# --- 2 ---
# Column month
user_info["month_purchase"] = user_info.purchase_time.apply(lambda x: calendar.month_name[x.month])

# --- 3 ---
# Column week
user_info["weekday_purchase"] = user_info.purchase_time.apply(lambda x: calendar.day_name[x.weekday()])

# --- 4 ---
# Column hour_of_the_day
user_info["hour_of_the_day"] = user_info.purchase_time.apply(lambda x: x.hour)

# --- 5 ---
# Column seconds_since_signup
user_info["seconds_since_signup"] = (user_info.purchase_time - user_info.signup_time).apply(lambda x: x.total_seconds())

# --- 6 ---
# Column countries_from_device (ie. number of different countries per device_id)
# We flag devices that committed purchases from different countries
country_count = user_info.groupby(by=["device_id", "IP_country"]).count().reset_index()
country_count = pd.DataFrame(country_count.groupby(by="device_id").count().IP_country)
user_info = user_info.merge(country_count, left_on="device_id", right_index=True)
user_info.rename(columns={"IP_country_x": "IP_country", "IP_country_y": "countries_from_device"}, inplace=True)

# Column "quick_purchase" : categorise time between sign_up and purchase
user_info["quick_purchase"] = user_info.seconds_since_signup.apply(lambda x: 1 if x < 30 else 0)

# age categorisation
user_info["age_category"] = user_info.age.apply(lambda x:
                                                "< 40" if x < 40 else
                                                "40 - 49" if x < 50 else
                                                "50 -59" if x < 60 else
                                                "60 - 69" if x < 70 else
                                                " > 70")

# Hour of the day categorisation
user_info["period_of_the_day"] = user_info.hour_of_the_day.apply(lambda x:
                                                                 "late night" if x < 4 else
                                                                 "early morning" if x < 8 else
                                                                 "morning" if x < 12 else
                                                                 "afternoon" if x < 16 else
                                                                 "evening" if x < 20 else
                                                                 "early night"
                                                                 )

user_info.head()

user_info.info()

"""## Logistic Regression"""

# Count missing values for each column
missing_counts = user_info.isnull().sum()

# Print the missing value counts
missing_counts

# Drop rows with any missing values.
user_info.dropna(inplace=True)

user_info.info()

# Specify the columns to drop.
columns_to_drop = ["user_id", "signup_time", "purchase_time", "device_id", "ip_address", "hour_of_the_day", "seconds_since_signup", "age"]

# Drop specified columns.
features = user_info.drop(columns=columns_to_drop)

# Display the updated dataframe.
print(features.head())

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import classification_report
from sklearn.metrics import roc_curve, roc_auc_score

target = features["class"]
features = features.drop(columns=["class"])

features.info()

# Splitting the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(features, target,
                                                    random_state=42,
                                                    stratify=target,
                                                    test_size=0.25)

import pandas as pd
# Identify categorical and numerical features
categorical_cols = features.select_dtypes(include=['object', 'category']).columns
numerical_cols = features.select_dtypes(include=['number']).columns

# One-hot encode categorical features
ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
X_train_cat = ohe.fit_transform(X_train[categorical_cols])
X_test_cat = ohe.transform(X_test[categorical_cols])

# Create dataframes from the encoded arrays
X_train_cat_df = pd.DataFrame(X_train_cat, columns=ohe.get_feature_names_out(categorical_cols))
X_test_cat_df = pd.DataFrame(X_test_cat, columns=ohe.get_feature_names_out(categorical_cols))

# Standard scale numerical features
scaler = StandardScaler()
X_train_num = scaler.fit_transform(X_train[numerical_cols])
X_test_num = scaler.transform(X_test[numerical_cols])

# Create dataframes from the scaled arrays
X_train_num_df = pd.DataFrame(X_train_num, columns=numerical_cols)
X_test_num_df = pd.DataFrame(X_test_num, columns=numerical_cols)

# Concatenate the encoded categorical and scaled numerical features
X_train_processed = pd.concat([X_train_cat_df, X_train_num_df], axis=1)
X_test_processed = pd.concat([X_test_cat_df, X_test_num_df], axis=1)

from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_processed, y_train)

# Fitting a logistic regression model
logistic_regression = LogisticRegression(solver='liblinear', random_state=42)
logistic_regression.fit(X_train_resampled, y_train_resampled)

# Printing scores
train_score = logistic_regression.score(X_train_resampled, y_train_resampled)
test_score = logistic_regression.score(X_test_processed, y_test)
print("Train Score:", round(train_score * 100, 2), "%")
print("Test Score:", round(test_score * 100, 2), "%")

# Predict probabilities on the test set
y_pred_prob = logistic_regression.predict_proba(X_test_processed)[:, 1]

# Set a custom threshold
custom_threshold = 0.5  # Example threshold, adjust as needed

# Convert probabilities to binary predictions based on the threshold
y_pred = (y_pred_prob >= custom_threshold).astype(int)

# Classification Report
print(classification_report(y_test, y_pred))

# Other metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
AUC_ROC= roc_auc_score(y_test, y_pred)

print(f"Accuracy: {accuracy}")
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1-score: {f1}")
print(f"AUC_ROC: {AUC_ROC}")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)

# You can visualize the confusion matrix using seaborn's heatmap
sns.heatmap(cm, annot=True, fmt="d",cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()